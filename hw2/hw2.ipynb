{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Извлечение коллокаций + NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (3 балла) Предложите 3 способа найти упоминания товаров в отзывах. **yes**\n",
    "\n",
    "2. (2 балла) Реализуйте один из предложенных вами способов.\n",
    "\n",
    "3. (1 балл) Соберите n-граммы с полученными сущностями (NE + левый сосед / NE + правый сосед)\n",
    "\n",
    "4. (3 балла) Ранжируйте n-граммы с помощью 3 коллокационных метрик (t-score, PMI и т.д.). Не забудьте про частотный фильтр / сглаживание. Выберите лучший результат (какая метрика  ранжирует выше коллокации, подходящие для отчёта).\n",
    "\n",
    "5. (1 балл) Сгруппируйте полученные коллокации по NE, выведите примеры для 5 товаров.\n",
    "\n",
    "\n",
    "Должны получиться примерно такие группы:\n",
    "```\n",
    "watch \n",
    "--- \n",
    "stylish watch\n",
    "good watches\n",
    "great watch\n",
    "love this watch\n",
    "...\n",
    "```\n",
    "\n",
    "**Бонус** (2 балла): \n",
    "если придумаете способ объединить синонимичные упоминания (например, \"Samsung Galaxy Watch\", \"watch\", \"smartwatch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Способы найти упоминания товаров в отзывах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ 1: Собственный W2V + мини-словарь\n",
    "\n",
    "1. На текстах всех отзывов обучаю свою модель W2V\n",
    "2. Вручную собираю небольшой список интересующих меня товаров ```serum, parfume, cleanser```\n",
    "3. Чтобы найти возможные упоминания сывороток смотрю на ближайших соседей ```serum```. Предполагаю, что они будут названиями продуктов, потому что должны были использоваться в тех же контекстах.\n",
    "\n",
    "#### Достоинства\n",
    "1. Очень гибкий способ получить названия, так он не привязан ни к каким шаблонам\n",
    "2. Кроме названий можно сразу получить качества упоминаемых товаров\n",
    "\n",
    "#### Недостатки\n",
    "1. Легко найти одноловные упоминания товаров, а двух и трех словные сложнее\n",
    "2. При маленьком корпусе результаты могут разочаровать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ 2: Метаданные + Комбинаторика\n",
    "\n",
    "1. Из метаданных беру все названия брендов ```Shiseido```\n",
    "2. В названиях товаров отсекаю называние бренда и получаю сами названия ```Shiseido White Lucent Brightening Moisturizing Emulsion``` --> ```White Lucent Brightening Moisturizing Emulsion```\n",
    "3. Из токенов названия получаю возможные названия ```White Lucent Brightening Moisturizing Emulsion```: ```Brightening Moisturizing Emulsion```, ```White Lucent Emulsion```, ```White Lucent```.\n",
    "\n",
    "#### Достоинства\n",
    "1. Получившиеся паттерны описывают конкретные товары\n",
    "2. Не требует много данных, достаточно списка бренда и товаров.\n",
    "\n",
    "#### Недостатки\n",
    "1. Основано на предположении, что в отзыве на товар будет указано его настоящее название или хотя бы его часть\n",
    "2. Плохо поддается генерализации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ 3: Заголовок отзыва + Шаблоны\n",
    "\n",
    "1. Беру все заголовки отзывов\n",
    "2. Шаблонами ищу в них именные группы ```Wonderful Skin Care Product``` -> Adj Noun Noun Noun -> ```Skin Care Product```\n",
    "\n",
    "#### Достоинства\n",
    "1. Паттерны быстро пишутся\n",
    "\n",
    "#### Недостатки\n",
    "1. Может быть много лишних матчей\n",
    "2. Привязка к морфлогочиескому таггеру, а их идеальных не бывает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Реализуйте один из предложенных вами способов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Я буду использовать первый способ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала преобразую [мои данные](http://snap.stanford.edu/data/amazon/Beauty.txt.gz) в один большой мета-документ, где нет пунктуации, цифр, стоп-слов, регистр нижний, каждое предложение на отдельной строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Beauty.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772616"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith('product/productId'):\n",
    "        continue\n",
    "    elif line.startswith('product/title'):\n",
    "        continue\n",
    "    elif line.startswith('product/price'):\n",
    "        continue\n",
    "    elif line.startswith('review/userId'):\n",
    "        continue\n",
    "    elif line.startswith('review/profileName'):\n",
    "        continue\n",
    "    elif line.startswith('review/helpfulness'):\n",
    "        continue\n",
    "    elif line.startswith('review/score'):\n",
    "        continue\n",
    "    elif line.startswith('review/time'):\n",
    "        continue\n",
    "    else:\n",
    "        content_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_content_lines = []\n",
    "for line in content_lines:\n",
    "    if line.startswith('review/summary:') or line.startswith('review/text:'):\n",
    "        clean_content_lines.append(line.split(':')[1])\n",
    "    else:\n",
    "        clean_content_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756168"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_content_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizavetaersova/miniconda3/lib/python3.6/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¡Мне пришлось прервать следующую ячейку на середине выполнения, потому что очень долго!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_sentences = []\n",
    "for line in clean_content_lines:\n",
    "    doc = nlp(line)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    all_sentences.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.txt', 'w+', encoding='utf-8') as f:\n",
    "    for sent in all_sentences:\n",
    "        f.write(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pattern\n",
    "from pattern.en import lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extended_stopwords_towards_nlp.txt', 'r', encoding='utf-8') as file:\n",
    "    raw = file.read()\n",
    "stopwords = raw.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text):\n",
    "    line = text.lower()\n",
    "    clean_text = re.sub(\"[^A-Za-z']\", ' ', line)\n",
    "    if len(clean_text) > 2:\n",
    "        line_lemmas = [lemma(wd) for wd in clean_text.split() if lemma(wd) not in stopwords]\n",
    "        lemmatized = \" \".join(line_lemmas)\n",
    "        if len(lemmatized) > 2:\n",
    "            return lemmatized\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 576 ms, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preproc_sent = []\n",
    "for sent in all_sentences:\n",
    "    clean_lemmas = preproc(sent)\n",
    "    if clean_lemmas is not None:\n",
    "        preproc_sent.append(clean_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вот так выглядит предобработанное сообщение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hair pretty curly short will frizz crazy'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_sent[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940984"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_doc.txt', 'w+', encoding='utf-8') as f:\n",
    "    for sent in preproc_sent[:100000]:\n",
    "        f.write(f\"{sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Натренирую собственный w2v на почти 1 млн предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'w2v_doc.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 1.32 s, total: 1min 51s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%time model_beauty = gensim.models.Word2Vec(data, size=300, max_vocab_size = 100000, window=3, min_count=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_beauty.init_sims(replace=True)\n",
    "model_path = \"beauty.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_beauty.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27413"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_beauty.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составлю список продуктов, для которых хочу делать поиск упоминаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = ['serum', 'moisturizer', 'cleanser', 'oil', 'perfume', 'sunscreen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найду самые близкие к ним слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для serum ближайшие соседи: [('masque', 0.718429446220398), ('booster', 0.6737878322601318), ('recovery', 0.6674602031707764), ('creme', 0.6610598564147949), ('retinol', 0.657066822052002), ('defense', 0.6453754901885986), ('mask', 0.6380693912506104), ('hydration', 0.6338380575180054), ('intensive', 0.632432222366333), ('vitamin', 0.6315939426422119)]\n",
      "Для moisturizer ближайшие соседи: [('moisturiser', 0.6949144005775452), ('cream', 0.6875280141830444), ('moisterizer', 0.6725209951400757), ('lotion', 0.6603338718414307), ('sunblock', 0.6538445949554443), ('moisiturizer', 0.6340392827987671), ('cleanser', 0.618161678314209), ('sunscreen', 0.6172863841056824), ('toner', 0.6126477718353271), ('spf', 0.609116792678833)]\n",
      "Для cleanser ближайшие соседи: [('toner', 0.7895275950431824), ('cleanse', 0.7720011472702026), ('astringent', 0.7232605814933777), ('exfoliant', 0.7053558826446533), ('exfoliator', 0.690445601940155), ('cetaphil', 0.6559640169143677), ('scrub', 0.6545692682266235), ('exfoliation', 0.6504665613174438), ('mask', 0.625485897064209), ('moisturizer', 0.6181617975234985)]\n",
      "Для oil ближайшие соседи: [('silicone', 0.5489289164543152), ('nutrient', 0.5396268367767334), ('extract', 0.5270925760269165), ('vitamin', 0.5010500550270081), ('coriander', 0.4893120527267456), ('queensland', 0.4886080026626587), ('hut', 0.485396146774292), ('oili', 0.4816563129425049), ('milk', 0.47699791193008423), ('alcohol', 0.46892035007476807)]\n",
      "Для perfume ближайшие соседи: [('cologne', 0.7486671209335327), ('fragrance', 0.7198485136032104), ('parfum', 0.6899582147598267), ('frangrance', 0.6684093475341797), ('parfume', 0.6483071446418762), ('perfum', 0.641375720500946), ('fragance', 0.6278813481330872), ('scent', 0.6094051599502563), ('ck', 0.5987077951431274), ('purfume', 0.5949117541313171)]\n",
      "Для sunscreen ближайшие соседи: [('sunblock', 0.8847535252571106), ('spf', 0.7856994867324829), ('solbar', 0.6926905512809753), ('oxide', 0.6276670694351196), ('zinc', 0.6174838542938232), ('moisturizer', 0.6172865033149719), ('vanicream', 0.611162543296814), ('moisturiser', 0.6035323143005371), ('protection', 0.5943726301193237), ('uva', 0.5879087448120117)]\n"
     ]
    }
   ],
   "source": [
    "possible_colloc = []\n",
    "for product in product_list:\n",
    "    closest = model_beauty.wv.most_similar(product, topn=10)\n",
    "    possible_colloc\n",
    "    print(f'Для {product} ближайшие соседи: {closest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Соберу n-граммы с полученными сущностями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти сочетания с ними запущу поиск триграмм, возьму из всех триграмм только те, где есть два нужных мне слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_doc.txt', 'r', encoding='utf-8') as f:\n",
    "    tokens = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 526 ms, total: 31.7 s\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ранжирую n-граммы с помощью 3 коллокационных метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'boost\", \"moussee'\", 'extrodinary'),\n",
       " (\"'goods'\", \"'could\", 'improved'),\n",
       " (\"'hyaluronic\", \"acid'\", 'purporte'),\n",
       " (\"'metro'\", \"'ryan\", \"seacrest'\"),\n",
       " ('aefjor', 'aroijmralkjierg', 'rgeeiojgpokgear'),\n",
       " ('aerosolize', 'airborne', 'assoc'),\n",
       " ('ajo', 'marisco', 'pescado'),\n",
       " ('amarelece', 'aconselho', 'vivamente'),\n",
       " ('amouage', 'lorenzo', 'villoresi'),\n",
       " ('aroijmralkjierg', 'rgeeiojgpokgear', 'esoijfgoij')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'thi', 'product'),\n",
       " ('recommend', 'thi', 'product'),\n",
       " ('buy', 'thi', 'product'),\n",
       " ('thi', 'product', 'year'),\n",
       " ('thi', 'product', 'work'),\n",
       " ('highly', 'recommend', 'thi'),\n",
       " ('love', 'thi', 'stuff'),\n",
       " ('love', 'thi', 'perfume'),\n",
       " ('love', 'love', 'love'),\n",
       " ('purchase', 'thi', 'product')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.student_t, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'boost\", \"moussee'\", 'extrodinary'),\n",
       " (\"'goods'\", \"'could\", 'improved'),\n",
       " (\"'hyaluronic\", \"acid'\", 'purporte'),\n",
       " (\"'metro'\", \"'ryan\", \"seacrest'\"),\n",
       " ('aefjor', 'aroijmralkjierg', 'rgeeiojgpokgear'),\n",
       " ('aerosolize', 'airborne', 'assoc'),\n",
       " ('ajo', 'marisco', 'pescado'),\n",
       " ('amarelece', 'aconselho', 'vivamente'),\n",
       " ('amouage', 'lorenzo', 'villoresi'),\n",
       " ('aroijmralkjierg', 'rgeeiojgpokgear', 'esoijfgoij')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.chi_sq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Группирую полученные коллокации по NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
